{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Web Scraping Code for Email Extraction\n",
    "\n",
    "This code performs web scraping to collect email addresses from two biological collection platforms: Specieslink and SIBBr. Below is a detailed breakdown of its functionality:\n",
    "\n",
    "1. **Installing Required Libraries**:\n",
    "   - The code starts by installing the necessary libraries, `requests` and `beautifulsoup4`, which are used for making HTTP requests and parsing HTML, respectively.\n",
    "\n",
    "2. **Importing Libraries**:\n",
    "   - After installation, the required libraries are imported into the script for use in subsequent operations.\n",
    "\n",
    "3. **Data Collection from Specieslink**:\n",
    "   - The code sends a GET request to the Specieslink URL to fetch the webpage content. If the request is successful, it parses the HTML and extracts data from a specified table, including links and other relevant texts.\n",
    "\n",
    "4. **Processing Extracted Data**:\n",
    "   - The extracted data is processed to split location information into separate columns and rename various columns for clarity. Unnecessary columns are dropped to streamline the DataFrame.\n",
    "\n",
    "5. **Email Extraction Functions**:\n",
    "   - Two functions, `get_emails_from_page(url)` and `get_emails_after_contact(url)`, are defined to extract email addresses from given URLs. The first function retrieves emails from the entire page text, while the second specifically looks for emails in the \"Contacts\" section or in \"Subcollections\" if the first section is absent.\n",
    "\n",
    "6. **Data Collection from SIBBr**:\n",
    "   - Another function, `extract_collection_info(url)`, is created to collect collection information from the SIBBr platform. It retrieves the list of collections available on the specified webpage.\n",
    "\n",
    "7. **Additional Information Retrieval**:\n",
    "   - The `get_additional_info(url)` function extracts additional details such as email addresses and acronyms from SIBBr pages.\n",
    "\n",
    "8. **Merging DataFrames**:\n",
    "   - DataFrames from Specieslink and SIBBr are merged based on collection names, combining relevant columns such as emails and acronyms. This step ensures that all collected data is consolidated in one place.\n",
    "\n",
    "9. **Updating Missing Emails**:\n",
    "   - The `update_missing_emails(df)` function iterates through the merged DataFrame to identify and fill in any missing email addresses by calling the appropriate extraction functions based on the link's domain.\n",
    "\n",
    "10. **Exporting Results**:\n",
    "    - Finally, the merged DataFrame, containing all collected information, is exported to a CSV file for further analysis or record-keeping.\n",
    "\n",
    "Overall, this code efficiently collects and organizes email addresses from biological collections, facilitating better communication and collaboration within the research community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of Required Libraries\n",
    "\n",
    "This cell installs the necessary libraries for web scraping:\n",
    "- `requests`: A simple library for making HTTP requests in Python.\n",
    "- `beautifulsoup4`: A library for parsing HTML and XML documents, commonly used for web scraping tasks.\n",
    "\n",
    "To execute this command, run the cell to ensure that both libraries are installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "In this cell, we import the essential libraries for the web scraping project:\n",
    "\n",
    "- `requests`: This library is used to send HTTP requests to web pages and fetch their content.\n",
    "  \n",
    "- `BeautifulSoup` from `bs4`: A powerful library for parsing HTML and XML documents. It allows us to navigate the parse tree and extract data easily.\n",
    "  \n",
    "- `pandas`: A library used for data manipulation and analysis. It provides data structures like DataFrames that are useful for organizing and storing data.\n",
    "\n",
    "- `re`: This is Python's built-in regular expressions library, which allows for pattern matching and text manipulation.\n",
    "\n",
    "These libraries will enable us to retrieve web content, parse it, and store the relevant information efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping from SpeciesLink\n",
    "\n",
    "In this cell, we perform web scraping on the SpeciesLink network map page to extract data from an HTML table. \n",
    "\n",
    "1. **Setting the URL**: \n",
    "   - We define the URL of the SpeciesLink network map.\n",
    "\n",
    "2. **Sending HTTP Request**: \n",
    "   - The `requests.get(url)` function sends a GET request to the specified URL. The response is stored in the `response` variable.\n",
    "\n",
    "3. **Checking Response Status**: \n",
    "   - We check if the request was successful by verifying if `response.status_code` is equal to 200.\n",
    "\n",
    "4. **Parsing the HTML Content**:\n",
    "   - If the request is successful, we parse the HTML content using `BeautifulSoup`, specifying the parser as `'html.parser'`.\n",
    "\n",
    "5. **Extracting Text and Tables**:\n",
    "   - We extract the entire text of the page using `soup.get_text()` and locate all `<table>` elements on the page with `soup.find_all('table')`.\n",
    "\n",
    "6. **Initializing Variables**:\n",
    "   - `table_index` is set to `0`, which indicates the first table on the page.\n",
    "   - We create an empty list, `links_texts`, to store extracted links and their corresponding text.\n",
    "\n",
    "7. **Processing the Table**:\n",
    "   - We check if there are tables available and proceed to extract data from the specified table index:\n",
    "     - We initialize empty lists for `headers` and `rows`.\n",
    "     - If a header row exists, we extract the headers from the table and store them in the `headers` list.\n",
    "     - We iterate over all rows in the table:\n",
    "       - For each row, we extract the text from all cells (`<td>` elements).\n",
    "       - We specifically process the second column to find any `<a>` tags (hyperlinks).\n",
    "       - For each hyperlink found, we construct the full URL by appending it to the base URL of SpeciesLink and store both the link and the display text in `links_texts`.\n",
    "       - We append the constructed link to the row of data.\n",
    "\n",
    "8. **Creating a DataFrame**:\n",
    "   - Finally, we create a `pandas` DataFrame, `df_specieslink`, to organize the extracted data into a structured format. If headers are present, they are used as the column names for the DataFrame; otherwise, the DataFrame is created without headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL to scrape\n",
    "url = 'https://specieslink.net/network-map'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "table_index = 0  # Index to select which table to extract (0 for the first table)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract the entire text from the page (not used later)\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Find all table elements on the page\n",
    "    tables = soup.find_all('table')\n",
    "    links_texts = []  # List to store links and their text\n",
    "\n",
    "    # Check if the specified table index exists in the list of tables\n",
    "    if len(tables) > table_index:\n",
    "        table = tables[table_index]  # Select the specified table\n",
    "        headers = []  # List to store table headers\n",
    "        rows = []     # List to store rows of data\n",
    "        \n",
    "        # Extract headers from the first row of the table, if they exist\n",
    "        header_row = table.find('tr')\n",
    "        if header_row:\n",
    "            headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "\n",
    "        # Iterate through each row in the table\n",
    "        for tr in table.find_all('tr'):\n",
    "            cells = tr.find_all('td')  # Find all cells in the current row\n",
    "            if len(cells) > 0:  # Only process rows with data (at least one cell)\n",
    "                row = [cell.get_text(strip=True) for cell in cells]  # Extract text from each cell\n",
    "                \n",
    "                second_column = cells[1]  # Select the second column to find links\n",
    "                # Find all <a> tags within the second column and extract their href attribute\n",
    "                for a_tag in second_column.find_all('a', href=True):\n",
    "                    link = 'https://specieslink.net' + a_tag['href']  # Construct the full URL\n",
    "                    text = a_tag.get_text(strip=True)  # Get the text for the link\n",
    "                    links_texts.append((link, text))  # Append the link and its text to the list\n",
    "                \n",
    "                row.append(link)  # Append the last extracted link to the current row\n",
    "                rows.append(row)  # Add the row data to the rows list\n",
    "                \n",
    "        # Create a DataFrame from the extracted table data\n",
    "        if headers:  # If headers exist, use them as column names\n",
    "            df_specieslink = pd.DataFrame(rows, columns=headers)\n",
    "        else:  # If no headers, create DataFrame without specified columns\n",
    "            df_specieslink = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the contents of the 4th column (index 3) into two new columns: 'Cidade' and 'Estado'\n",
    "# The contents are split based on ' / ' delimiter and expanded into separate columns\n",
    "df_specieslink[['Cidade', 'Estado']] = df_specieslink[3].str.split(' / ', expand=True)\n",
    "\n",
    "# Renaming the columns of the DataFrame for better readability\n",
    "# The current index numbers correspond to the columns that need renaming\n",
    "df_specieslink = df_specieslink.rename(columns={1: 'sigla', 2: 'nome', 4: 'ano', 6: 'link_specieslink'})\n",
    "\n",
    "# Dropping unnecessary columns from the DataFrame\n",
    "# Columns at index 0, 3, and 5 are removed as they are not needed for further analysis\n",
    "df_specieslink = df_specieslink.drop(columns=[0, 3, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails_from_page(url):\n",
    "    \"\"\"\n",
    "    Retrieves unique email addresses from the specified web page URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The web page URL from which email addresses will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of unique email addresses found on the web page.\n",
    "                If the URL is inaccessible or an error occurs, an empty list is returned.\n",
    "\n",
    "    Exceptions:\n",
    "    The function handles exceptions that may arise during the HTTP request or HTML parsing. \n",
    "    If an error occurs, a message is printed to indicate the problem, \n",
    "    and an empty list is returned.\n",
    "\n",
    "    Usage Example:\n",
    "    >>> url = \"https://example.com\"\n",
    "    >>> emails = get_emails_from_page(url)\n",
    "    >>> print(emails)  # Prints the list of unique email addresses found on the page.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses a regular expression to match standard email formats. \n",
    "      The pattern it uses is designed to capture most valid email addresses but may not account for all possible formats.\n",
    "    - Ensure that the URL provided is publicly accessible and allows for scraping.\n",
    "    \"\"\"\n",
    "    # This function retrieves unique email addresses from the specified web page URL\n",
    "    try:\n",
    "        # Sending a GET request to the provided URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the response status code indicates success (200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Extract all text from the page\n",
    "            page_text = soup.get_text()\n",
    "            \n",
    "            # Define a regular expression pattern to match email addresses\n",
    "            email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "            # Find all email addresses in the page text\n",
    "            emails = email_pattern.findall(page_text)\n",
    "            # Use a set to remove duplicate email addresses\n",
    "            unique_emails = set(emails)\n",
    "            # Return the unique email addresses as a list\n",
    "            return list(unique_emails)\n",
    "        else:\n",
    "            # Print an error message if the request was unsuccessful\n",
    "            print(f'Falha ao acessar a URL {url}. Status code: {response.status_code}')\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that occur during the request\n",
    "        print(f'Erro ao acessar a URL {url}: {e}')\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails_after_contact(url):\n",
    "    \"\"\"\n",
    "    Extracts unique email addresses from the \"Contatos\" section of a specified web page URL. \n",
    "    If the \"Contatos\" section is not found, it attempts to find email addresses in the \"Subcoleções\" section \n",
    "    and updates the main DataFrame with new links.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The web page URL from which email addresses and subcollection information will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of unique email addresses found in the \"Contatos\" section.\n",
    "                If no emails are found, or if neither section is present, it returns an empty list.\n",
    "                Additionally, if subcollection links are found, a DataFrame with new links is returned.\n",
    "\n",
    "    Exceptions:\n",
    "    This function handles exceptions that may arise during the HTTP request or HTML parsing. \n",
    "    If an error occurs, a message is printed to indicate the problem, \n",
    "    and an empty list is returned.\n",
    "\n",
    "    Usage Example:\n",
    "    >>> url = \"https://example.com\"\n",
    "    >>> emails = get_emails_after_contact(url)\n",
    "    >>> print(emails)  # Prints the list of unique email addresses found in the \"Contatos\" section.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses a regular expression to match standard email formats. \n",
    "      The pattern it uses is designed to capture most valid email addresses but may not account for all possible formats.\n",
    "    - If the \"Contatos\" section is absent, it checks for the \"Subcoleções\" section and extracts relevant links. \n",
    "      The main DataFrame `df_specieslink` is updated with any new links found.\n",
    "    - Ensure that the URL provided is publicly accessible and allows for scraping.\n",
    "    \"\"\"\n",
    "    global df_specieslink\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Encontra a tag <h4> com o texto \"Contatos\"\n",
    "            contact_header = soup.find('h4', string='Contatos')\n",
    "            \n",
    "            if contact_header:\n",
    "                # Pega o próximo elemento após a tag <h4>\n",
    "                siblings = contact_header.find_next_siblings()\n",
    "                \n",
    "                # Junte o texto de todos os irmãos até encontrar um email\n",
    "                section_text = ''\n",
    "                for sibling in siblings:\n",
    "                    section_text += sibling.get_text() + ' '\n",
    "                    \n",
    "                # Expressão regular para encontrar e-mails\n",
    "                email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "                emails = email_pattern.findall(section_text)\n",
    "                \n",
    "                # Remove e-mails duplicados\n",
    "                unique_emails = set(emails)\n",
    "                if unique_emails:\n",
    "                    return list(unique_emails)\n",
    "                else:\n",
    "                    print(f'Não foi possível encontrar e-mails na seção após a tag <h4> \"Contatos\" em {url}.')\n",
    "                    return []\n",
    "            else:\n",
    "                print(f'Tag <h4> com o texto \"Contatos\" não encontrada em {url}.')\n",
    "                \n",
    "                # Procura \"Subcoleções\" se \"Contatos\" não for encontrado\n",
    "                subcollections_header = soup.find('h4', string='Subcoleções')\n",
    "                \n",
    "                if subcollections_header:\n",
    "                    list_section = subcollections_header.find_next_sibling()\n",
    "                    \n",
    "                    if list_section:\n",
    "                        subcollections_info = []\n",
    "                        \n",
    "                        for li in list_section.find_all('li'):\n",
    "                            a_tag = li.find('a', href=True)\n",
    "                            \n",
    "                            if a_tag:\n",
    "                                sigla = a_tag.get_text(strip=True)\n",
    "                                link = 'https://specieslink.net' + a_tag['href']\n",
    "                                name = li.get_text(strip=True).replace(sigla, '').strip().replace('- ', '')\n",
    "                                \n",
    "                                subcollections_info.append({'nome': name, 'sigla': sigla, 'link_specieslink': link})\n",
    "                        \n",
    "                        # Cria um DataFrame com os links coletados\n",
    "                        new_links_df = pd.DataFrame(subcollections_info)\n",
    "                        \n",
    "                        # Remove links que já estão no DataFrame principal\n",
    "                        existing_links = set(df_specieslink['link_specieslink'])\n",
    "                        new_links_df = new_links_df[~new_links_df['link_specieslink'].isin(existing_links)]\n",
    "                        df_specieslink = pd.concat([df_specieslink, new_links_df], ignore_index=True)\n",
    "                        return new_links_df\n",
    "                else:\n",
    "                    print(f'Tag <h4> com o texto \"Subcoleções\" não encontrada em {url}.')\n",
    "                    return get_emails_from_page(url)  # Chama a função para extrair e-mails diretamente\n",
    "        else:\n",
    "            print(f'Falha ao acessar a URL {url}. Status code: {response.status_code}')\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao acessar a URL {url}: {e}')\n",
    "        return []\n",
    "\n",
    "def get_emails_from_page(url):\n",
    "    \"\"\"\n",
    "    Extracts unique email addresses from the specified web page URL.\n",
    "\n",
    "    This function sends a GET request to the provided URL, \n",
    "    retrieves the page content, and uses a regular expression \n",
    "    to find all email addresses present in the text of the page.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The web page URL from which email addresses will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of unique email addresses found on the page. \n",
    "               If no emails are found or if there is an error accessing the URL, an empty list is returned.\n",
    "\n",
    "    Exceptions:\n",
    "    This function handles exceptions that may arise during the HTTP request or HTML parsing. \n",
    "    If an error occurs, a message is printed to indicate the problem, \n",
    "    and an empty list is returned.\n",
    "\n",
    "    Usage Example:\n",
    "    >>> url = \"https://example.com\"\n",
    "    >>> emails = get_emails_from_page(url)\n",
    "    >>> print(emails)  # Prints the list of unique email addresses found on the page.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses a regular expression to match standard email formats. \n",
    "      The pattern it uses is designed to capture most valid email addresses but may not account for all possible formats.\n",
    "    - Ensure that the URL provided is publicly accessible and allows for scraping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page_text = soup.get_text()\n",
    "            email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "            emails = email_pattern.findall(page_text)\n",
    "            unique_emails = set(emails)\n",
    "            return list(unique_emails)\n",
    "        else:\n",
    "            print(f'Falha ao acessar a URL {url}. Status code: {response.status_code}')\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao acessar a URL {url}: {e}')\n",
    "        return []\n",
    "\n",
    "df_specieslink['email'] = df_specieslink['link_specieslink'].apply(lambda url: ', '.join(get_emails_after_contact(url)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specieslink.to_csv('email_list1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://collectory.sibbr.gov.br/collectory/'\n",
    "\n",
    "def extract_collection_info(url):\n",
    "    \"\"\"\n",
    "    Extracts information about collections from the specified SIBBR Collectory URL.\n",
    "\n",
    "    This function sends a GET request to the provided SIBBR Collectory URL, \n",
    "    retrieves the page content, and extracts the names and links of collections \n",
    "    listed in the filtered collection list.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the SIBBR Collectory page from which collection information will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, str]]: A list of dictionaries, each containing the name and link of a collection.\n",
    "                          If no collections are found or if there is an error accessing the URL, \n",
    "                          an empty list is returned.\n",
    "\n",
    "    Exceptions:\n",
    "    This function handles exceptions that may arise during the HTTP request or HTML parsing. \n",
    "    If an error occurs, a message is printed to indicate the problem, \n",
    "    and an empty list is returned.\n",
    "\n",
    "    Usage Example:\n",
    "    >>> url = \"https://collectory.sibbr.gov.br/collectory/\"\n",
    "    >>> collections = extract_collection_info(url)\n",
    "    >>> print(collections)  # Prints a list of collections with their names and links.\n",
    "\n",
    "    Notes:\n",
    "    - The function specifically looks for a <ul> element with the id 'filtered-list' \n",
    "      and extracts all <li> items within it. \n",
    "      Each <li> item is expected to contain an <a> tag with the collection name and URL.\n",
    "    - Ensure that the provided URL is publicly accessible and allows for scraping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find tag <ul> with id 'filtered-list'\n",
    "            filtered_list = soup.find('ul', id='filtered-list')\n",
    "            \n",
    "            if filtered_list:\n",
    "                collections = []\n",
    "                \n",
    "                # Find all tags <li> in <ul>\n",
    "                for li in filtered_list.find_all('li'):\n",
    "                    a_tag = li.find('a', href=True)\n",
    "                    \n",
    "                    if a_tag:\n",
    "                        collection_name = a_tag.get_text(strip=True)\n",
    "                        collection_url = 'https://collectory.sibbr.gov.br' + a_tag['href']\n",
    "                        collections.append({'nome': collection_name, 'link_sibbr': collection_url})\n",
    "                \n",
    "                return collections\n",
    "            else:\n",
    "                print(f\"Tag <ul> com o id 'filtered-list' não encontrada em {url}.\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f'Falha ao acessar a URL {url}. Status code: {response.status_code}')\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao acessar a URL {url}: {e}')\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = extract_collection_info(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_info(url):\n",
    "    \"\"\"\n",
    "    Retrieves additional information (emails and acronym) from the specified URL.\n",
    "\n",
    "    This function performs an HTTP GET request to the given URL, extracts email \n",
    "    addresses from the page's content, and looks for a specific acronym \n",
    "    contained within a designated HTML element.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the page to scrape for additional information.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[str, str]: A tuple containing:\n",
    "        - email_list (str): A comma-separated string of unique email addresses found on the page.\n",
    "        - acronym_text (str): The acronym extracted from the page, or an empty string if not found.\n",
    "\n",
    "    Example:\n",
    "    - If the URL is valid and contains emails and an acronym, the function will return\n",
    "      the emails as a string and the acronym as a string. If no emails or acronym \n",
    "      are found, it will return an empty string for each.\n",
    "\n",
    "    Note:\n",
    "    - The function handles errors related to network requests and parsing.\n",
    "    - It looks for emails in two places: as plain text on the page and encoded in \n",
    "      JavaScript (within `onclick` attributes).\n",
    "    \"\"\"\n",
    "    # Try to get the response from the URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract emails from the page text\n",
    "            page_text = soup.get_text()\n",
    "            email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "            emails = email_pattern.findall(page_text)\n",
    "\n",
    "            # Extract emails from the contact div if it exists\n",
    "            contact_div = soup.find('div', class_='contact')\n",
    "            if contact_div:\n",
    "                a_tags = contact_div.find_all('a', href=True)\n",
    "                for a_tag in a_tags:\n",
    "                    onclick_text = a_tag.get('onclick', '')\n",
    "                    email_match = re.search(r\"sendEmail\\('([^']*)'\\)\", onclick_text)\n",
    "                    if email_match:\n",
    "                        email_encoded = email_match.group(1)\n",
    "                        email = email_encoded.replace('(SPAM_MAIL@ALA.ORG.AU)', '@')\n",
    "                        emails.append(email)\n",
    "\n",
    "            # Get unique emails\n",
    "            unique_emails = set(emails)\n",
    "            email_list = ', '.join(unique_emails)\n",
    "\n",
    "            # Extract acronym from the page\n",
    "            acronym_tag = soup.find('span', class_='acronym')\n",
    "            acronym_text = acronym_tag.get_text(strip=True).replace('Acronym: ', '') if acronym_tag else ''\n",
    "\n",
    "            return email_list, acronym_text\n",
    "        else:\n",
    "            print(f'Falha ao acessar a URL {url}. Status code: {response.status_code}')\n",
    "            return '', ''\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao acessar a URL {url}: {e}')\n",
    "        return '', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through collection_info to retrieve additional information\n",
    "for collection in collection_info:\n",
    "    email, acronym = get_additional_info(collection['link_sibbr'])\n",
    "    collection['email'] = email\n",
    "    collection['sigla'] = acronym\n",
    "\n",
    "# Create a DataFrame from the collection information\n",
    "df_sibbr = pd.DataFrame(collection_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame df_sibbr to a CSV file named 'email_list2.csv'\n",
    "df_sibbr.to_csv('email_list2.csv')\n",
    "\n",
    "# Merge df_specieslink and df_sibbr on the 'nome' column\n",
    "# The merge is performed as an outer join to include all records from both DataFrames\n",
    "# Suffixes are added to distinguish columns from each DataFrame in case of overlap\n",
    "merged_df = pd.merge(df_specieslink, df_sibbr, on='nome', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# Combine the 'sigla' columns from both DataFrames\n",
    "# The combine_first method fills missing values in 'sigla_df1' with values from 'sigla_df2'\n",
    "merged_df['sigla'] = merged_df['sigla_df1'].combine_first(merged_df['sigla_df2'])\n",
    "\n",
    "# Combine the 'email' columns from both DataFrames\n",
    "# The combine_first method fills missing values in 'email_df1' with values from 'email_df2'\n",
    "merged_df['email'] = merged_df['email_df1'].combine_first(merged_df['email_df2'])\n",
    "\n",
    "# Combine the 'link' columns from both DataFrames\n",
    "# The combine_first method fills missing values in 'link_sibbr' with values from 'link_specieslink'\n",
    "merged_df['link'] = merged_df['link_sibbr'].combine_first(merged_df['link_specieslink'])\n",
    "\n",
    "# Remove the original columns that were used for merging and combining\n",
    "merged_df = merged_df.drop(columns=['sigla_df1', 'sigla_df2', 'email_df1', 'email_df2'])\n",
    "\n",
    "# Create a new DataFrame containing records with missing email addresses\n",
    "missing_emails_df = merged_df[merged_df['email'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_missing_emails(df):\n",
    "    \"\"\"\n",
    "    Updates missing email addresses in the DataFrame by extracting them from the corresponding links.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing collection information, including links and email addresses.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The updated DataFrame with missing email addresses filled in.\n",
    "    \"\"\"\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        email = row['email']  # Get the current email for the row\n",
    "        link = row['link']    # Get the current link for the row\n",
    "        \n",
    "        # Check if the email is empty or contains only whitespace\n",
    "        if pd.isna(email) or email.strip() == '':\n",
    "            # If the link belongs to specieslink.net, extract emails using the corresponding function\n",
    "            if 'specieslink.net' in link:\n",
    "                emails = get_emails_after_contact(link)\n",
    "            # If the link belongs to sibbr, extract emails and acronym using the corresponding function\n",
    "            elif 'sibbr' in link:\n",
    "                emails, acronym = get_additional_info(link)\n",
    "            else:\n",
    "                emails = []  # If the link doesn't match any known source, set emails to an empty list\n",
    "\n",
    "            # If emails are found, update the DataFrame with the new emails\n",
    "            if emails:\n",
    "                df.at[index, 'email'] = ', '.join(emails)  # Join multiple emails into a single string\n",
    "\n",
    "    return df  # Return the updated DataFrame\n",
    "\n",
    "# Update the main DataFrame with missing emails\n",
    "merged_df = update_missing_emails(merged_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
